{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05808d2e-85fb-4ec7-ace8-23a445906845",
   "metadata": {},
   "source": [
    "# PySpark data loading and preliminary cleaning complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa27e429-31c7-45d8-949b-95e39344be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, to_timestamp\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db88fc8e-9a7f-4d17-9663-903453436b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/16 22:28:11 WARN Utils: Your hostname, MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.2.100 instead (on interface en0)\n",
      "25/07/16 22:28:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/16 22:28:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Datetime: timestamp (nullable = false)\n",
      " |-- Global_active_power: double (nullable = true)\n",
      " |-- Global_reactive_power: double (nullable = true)\n",
      " |-- Voltage: double (nullable = true)\n",
      " |-- Global_intensity: double (nullable = true)\n",
      " |-- Sub_metering_1: double (nullable = true)\n",
      " |-- Sub_metering_2: double (nullable = true)\n",
      " |-- Sub_metering_3: double (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n",
      "|Datetime           |Global_active_power|Global_reactive_power|Voltage|Global_intensity|Sub_metering_1|Sub_metering_2|Sub_metering_3|\n",
      "+-------------------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n",
      "|2006-12-16 17:24:00|4.216              |0.418                |234.84 |18.4            |0.0           |1.0           |17.0          |\n",
      "|2006-12-16 17:25:00|5.36               |0.436                |233.63 |23.0            |0.0           |1.0           |16.0          |\n",
      "|2006-12-16 17:26:00|5.374              |0.498                |233.29 |23.0            |0.0           |2.0           |17.0          |\n",
      "|2006-12-16 17:27:00|5.388              |0.502                |233.74 |23.0            |0.0           |1.0           |17.0          |\n",
      "|2006-12-16 17:28:00|3.666              |0.528                |235.68 |15.8            |0.0           |1.0           |17.0          |\n",
      "+-------------------+-------------------+---------------------+-------+----------------+--------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 22:28:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/07/16 22:28:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/07/16 22:28:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "[Stage 2:>                                                        (0 + 10) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed and saved to: ../data/processed/cleaned_power.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 22:28:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/07/16 22:28:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Electric Power Project - Data Cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# File path\n",
    "file_path = \"../data/raw/household_power.csv\"\n",
    "\n",
    "# Load data (semicolon-separated, '?' as missing)\n",
    "df_raw = spark.read.csv(file_path, header=True, sep=\";\", inferSchema=False)\n",
    "\n",
    "# Replace '?' with null and drop rows with nulls\n",
    "df = df_raw.replace(\"?\", None).dropna()\n",
    "\n",
    "# Cast numeric columns to DoubleType\n",
    "columns_to_cast = [\n",
    "    \"Global_active_power\", \"Global_reactive_power\", \"Voltage\",\n",
    "    \"Global_intensity\", \"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"\n",
    "]\n",
    "for col_name in columns_to_cast:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "# Create timestamp column from Date and Time\n",
    "df = df.withColumn(\"Datetime\", to_timestamp(concat_ws(\" \", col(\"Date\"), col(\"Time\")), \"d/M/yyyy H:mm:ss\"))\n",
    "\n",
    "# Drop original Date and Time columns\n",
    "df = df.drop(\"Date\", \"Time\")\n",
    "\n",
    "# Reorder columns (Datetime first)\n",
    "cols = df.columns\n",
    "cols.remove(\"Datetime\")\n",
    "df = df.select([\"Datetime\"] + cols)\n",
    "\n",
    "# Show schema and sample data\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Save cleaned data to Parquet\n",
    "output_path = \"../data/processed/cleaned_power.parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(\"Data cleaning completed and saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bd565-95ad-4b66-bfe2-490b1e96b3f8",
   "metadata": {},
   "source": [
    "# Aggregate data to daily level using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528358dc-27c4-4b53-975b-d654bbe5699b",
   "metadata": {},
   "source": [
    "Convert the cleaned minute-level electricity data into daily-level aggregated data to be used in time series forecasting and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41640f8d-7278-489b-a5c4-5141499a207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/16 22:32:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+-----------+-----------+------------------+---------+---------+---------+\n",
      "|      Date|  avg_active_power|total_active_power|       avg_voltage|max_voltage|min_voltage|     avg_intensity|sum_sub_1|sum_sub_2|sum_sub_3|\n",
      "+----------+------------------+------------------+------------------+-----------+-----------+------------------+---------+---------+---------+\n",
      "|2006-12-16|3.0534747474747492|1209.1760000000006|236.24376262626276|     243.73|     230.98|13.082828282828302|      0.0|    546.0|   4926.0|\n",
      "|2006-12-17| 2.354486111111111|           3390.46|240.08702777777793|     249.37|     229.57| 9.999027777777764|   2033.0|   4187.0|  13341.0|\n",
      "|2006-12-18|1.5304347222222197|2203.8259999999964|241.23169444444474|     248.48|     229.08| 6.421666666666658|   1063.0|   2621.0|  14018.0|\n",
      "|2006-12-19| 1.157079166666667|1666.1940000000006|241.99931250000026|     248.89|     231.24| 4.926388888888899|    839.0|   7602.0|   6197.0|\n",
      "|2006-12-20|1.5456583333333327| 2225.747999999999|242.30806250000026|     249.48|     233.43|6.4673611111111065|      0.0|   2648.0|  14063.0|\n",
      "+----------+------------------+------------------+------------------+-----------+-----------+------------------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "Daily-level data saved to: ../data/processed/daily_power.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, avg, max, min, sum as spark_sum\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Electric Power Project - Daily Aggregation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the cleaned Parquet data\n",
    "input_path = \"../data/processed/cleaned_power.parquet\"\n",
    "df = spark.read.parquet(input_path)\n",
    "\n",
    "# Extract date from timestamp\n",
    "df = df.withColumn(\"Date\", to_date(\"Datetime\"))\n",
    "\n",
    "# Aggregate data by date using common statistical functions\n",
    "df_daily = df.groupBy(\"Date\").agg(\n",
    "    avg(\"Global_active_power\").alias(\"avg_active_power\"),\n",
    "    spark_sum(\"Global_active_power\").alias(\"total_active_power\"),\n",
    "    avg(\"Voltage\").alias(\"avg_voltage\"),\n",
    "    max(\"Voltage\").alias(\"max_voltage\"),\n",
    "    min(\"Voltage\").alias(\"min_voltage\"),\n",
    "    avg(\"Global_intensity\").alias(\"avg_intensity\"),\n",
    "    spark_sum(\"Sub_metering_1\").alias(\"sum_sub_1\"),\n",
    "    spark_sum(\"Sub_metering_2\").alias(\"sum_sub_2\"),\n",
    "    spark_sum(\"Sub_metering_3\").alias(\"sum_sub_3\")\n",
    ")\n",
    "\n",
    "# Optional: sort by date\n",
    "df_daily = df_daily.orderBy(\"Date\")\n",
    "\n",
    "# Show results\n",
    "df_daily.show(5)\n",
    "\n",
    "# Save to Parquet for modeling\n",
    "output_path = \"../data/processed/daily_power.parquet\"\n",
    "df_daily.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(\"Daily-level data saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad37820-1d69-42fd-9b6b-0d93b3e48165",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) on Daily Power Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2da033-a908-4658-a23d-8dc2a27f666d",
   "metadata": {},
   "source": [
    "Use Pandas and Matplotlib/Seaborn to perform basic exploratory data analysis (EDA) on the daily-level electricity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df9e996-54a2-4116-a5da-efd392208cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/April/anaconda3/envs/bigdata_project/lib/python3.10/site-packages (20.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b941332-7358-447a-b323-410ab5f0ce68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the daily-level Parquet file (converted by PySpark)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed/daily_power.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/daily_power.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Show basic info\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Data shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigdata_project/lib/python3.10/site-packages/pandas/io/parquet.py:653\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    502\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    656\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/bigdata_project/lib/python3.10/site-packages/pandas/io/parquet.py:68\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     66\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Load the daily-level Parquet file (converted by PySpark)\n",
    "file_path = \"../data/processed/daily_power.parquet\"\n",
    "df = pd.read_parquet(\"../data/processed/daily_power.parquet\")\n",
    "\n",
    "# Show basic info\n",
    "print(\"ðŸ“Š Data shape:\", df.shape)\n",
    "print(\"ðŸ§¾ Columns:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# Convert 'Date' column to datetime (if not already)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Plot total active power over time\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df['Date'], df['total_active_power'], color='blue')\n",
    "plt.title(\"Total Daily Active Power (kW)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Active Power\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot average voltage over time\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df['Date'], df['avg_voltage'], color='orange')\n",
    "plt.title(\"Average Daily Voltage (V)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Avg Voltage\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of sub-metering consumption\n",
    "df_sub = df[['sum_sub_1', 'sum_sub_2', 'sum_sub_3']]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(df_sub.corr(), annot=True, cmap=\"Blues\")\n",
    "plt.title(\"Correlation: Sub-metering\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram of total daily active power\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df['total_active_power'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Total Daily Active Power\")\n",
    "plt.xlabel(\"Total Active Power (kW)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a3fcd-3dc4-4f5b-99f6-09746c773523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
